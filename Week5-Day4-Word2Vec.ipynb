{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-13T02:48:50.700269Z",
     "start_time": "2017-10-13T02:48:50.683737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@media print {\n",
       "  a[href]:after {\n",
       "    content: none !important;\n",
       "  }\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext do_not_print_href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram) in Tensorflow\n",
    "\n",
    "\n",
    "## 참고자료\n",
    "\n",
    "- Google Word2Vec Publications\n",
    "  - \"Efficient Estimation of Word Representations in Vector Space\"\n",
    "    - https://arxiv.org/abs/1301.3781\n",
    "  - \"Distributed Representations of Words and Phrases and their Compositionality\", Mikolov, et. al.\n",
    "    - https://arxiv.org/abs/1310.4546\n",
    "\n",
    "\n",
    "- word2vec 관련 이론 정리, 김범석, 블로그 포스트\n",
    "  - https://shuuki4.wordpress.com/2016/01/27/word2vec-관련-이론-정리/\n",
    "\n",
    "\n",
    "- 한국어와 NLTK, Gensim의 만남, 박은정, PyCon Korea 2015\n",
    "  - https://www.lucypark.kr/slides/2015-pyconkr/\n",
    "\n",
    "\n",
    "- Stanford Univ CS20SI 강의 예제\n",
    "  - https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/examples/04_word2vec_visualize.py\n",
    "\n",
    "\n",
    "- 텐서플로우 튜토리얼\n",
    "  - https://www.tensorflow.org/tutorials/word2vec\n",
    "\n",
    "\n",
    "- 구글 그룹스 word2vec-toolkit\n",
    "  - https://groups.google.com/forum/#!forum/word2vec-toolkit\n",
    "\n",
    "\n",
    "  \n",
    "## 개념 : 단어를 실수의 다차원 벡터로 표현\n",
    "  - 단어의 갯수 (V) 는 고정, 예를 들어 50000 ~ 100000 정도의 크기\n",
    "  - 단어를 표현하는 차원수 (N) 는 100 ~ 1000 차원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 : Auto-Encoder 와 유사한 구조의 모델을 이용\n",
    "  - input layer\n",
    "    - 단어의 인덱스값의 one-hot encoding\n",
    "  - hidden layer\n",
    "    - 단어의 임베딩을 출력 (N개 유니트)\n",
    "  - output layer \n",
    "    - 단어의 one-hot encoding 을 출력\n",
    "    - 목적하는 단어의 one-hot vector 와 cross-entropy loss 로 학습\n",
    "  - 원하는 결과는 hidden layer 출력값을 학습하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"screenshot-23.54.41.png\" style=\"width:55.5rem\"/>\n",
    "\n",
    "이미지출처: https://youtu.be/BD8wPsr_DAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"igSuE.png\" style=\"width:45.5rem\"/>\n",
    "\n",
    "이미지출처: https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input layer to hidden layer weight parameter\n",
    "\n",
    "  - $V \\times N$ : 5,000,000 ~ 100,000,000\n",
    "\n",
    "```\n",
    "    # embed_matrix : VOCAB_SIZE x EMBED_SIZE\n",
    "    self.embed_matrix = tf.Variable(\n",
    "        tf.random_uniform(\n",
    "            [self.vocab_size, \n",
    "             self.embed_size],\n",
    "            -init_range,\n",
    "            init_range), \n",
    "        name='embed_matrix')\n",
    "        \n",
    "```\n",
    "\n",
    "### hidden to output layer weight parameter\n",
    "\n",
    "  - $N \\times V$ : 5,000,000 ~ 100,000,000\n",
    "\n",
    "```\n",
    "    # hidden_to_output weight : EMBED_SIZE x VOCAB_SIZE\n",
    "    nce_weight = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [self.vocab_size, self.embed_size],\n",
    "            stddev=1.0 / (self.embed_size ** 0.5)), \n",
    "        name='nce_weight')\n",
    "    nce_bias = tf.Variable(\n",
    "        tf.zeros([VOCAB_SIZE]),\n",
    "        name='nce_bias')\n",
    "        \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩에 최적화된 연산을 이용\n",
    "\n",
    "  - [`tf.nn.embedding_lookup()`](http://devdocs.io/tensorflow~python/tf/nn/embedding_lookup)\n",
    "    - <span style=\"color:red\">주의: `embedding_lookup()` 을 GPU 에 놓는 것은 텐서플로우에서 지원하지 않습니다.</span>\n",
    "    - `with tf.device('/cpu:0'):`  과 같은 코드를 이용해서 CPU 에서 실행되도록 할 필요가 있음\n",
    "  - [`tf.nn.nce_loss()`](http://devdocs.io/tensorflow~python/tf/nn/nce_loss)\n",
    "\n",
    "\n",
    "### [`tf.nn.embedding_lookup()`](http://devdocs.io/tensorflow~python/tf/nn/embedding_lookup)\n",
    "\n",
    "```\n",
    "    embedding_lookup(\n",
    "        params,\n",
    "        ids,\n",
    "        partition_strategy='mod',\n",
    "        name=None,\n",
    "        validate_indices=True,\n",
    "        max_norm=None\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "### [`tf.nn.nce_loss()`](http://devdocs.io/tensorflow~python/tf/nn/nce_loss)\n",
    "\n",
    "```\n",
    "    nce_loss(\n",
    "        weights,\n",
    "        biases,\n",
    "        labels,\n",
    "        inputs,\n",
    "        num_sampled,\n",
    "        num_classes,\n",
    "        num_true=1,\n",
    "        sampled_values=None,\n",
    "        remove_accidental_hits=False,\n",
    "        partition_strategy='mod',\n",
    "        name='nce_loss'\n",
    "    )\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그러면 오류 역전파 (back propagation) 는 어떻게 하나\n",
    "\n",
    "  - embedding_lookup() 연산의 back propagation 시의 gradients 는 다음 연산들에 대한 gradients 계산을 이용해서 구현되어 있다\n",
    "    - [`tf.gather()`](http://devdocs.io/tensorflow~python/tf/gather)\n",
    "    - [`tf.sparse_segment_sum()`](http://devdocs.io/tensorflow~python/tf/sparse_segment_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 모형 구성\n",
    "\n",
    "### placeholders\n",
    "\n",
    "```\n",
    "    self.center_words = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape=[self.batch_size],\n",
    "        name='center_words')\n",
    "    self.target_words = tf.placeholder(\n",
    "        tf.int32,\n",
    "        shape=[self.batch_size, 1],\n",
    "        name='target_words')\n",
    "\n",
    "```\n",
    "\n",
    "### variables\n",
    "\n",
    "```\n",
    "    init_range = 0.5 / self.embed_size\n",
    "    self.embed_matrix = tf.Variable(\n",
    "        tf.random_uniform(\n",
    "            [self.vocab_size, \n",
    "             self.embed_size],\n",
    "            -init_range,\n",
    "            init_range), \n",
    "        name='embed_matrix')\n",
    "\n",
    "    nce_weight = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            [self.vocab_size, self.embed_size],\n",
    "            stddev=1.0 / (self.embed_size ** 0.5)), \n",
    "        name='nce_weight')\n",
    "    nce_bias = tf.Variable(\n",
    "        tf.zeros([VOCAB_SIZE]),\n",
    "        name='nce_bias')\n",
    "\n",
    "```\n",
    "\n",
    "### operations\n",
    "\n",
    "```\n",
    "    embed = tf.nn.embedding_lookup(\n",
    "        self.embed_matrix,\n",
    "        self.center_words,\n",
    "        name='embed')\n",
    "\n",
    "    self.loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weight, \n",
    "            biases=nce_bias, \n",
    "            labels=self.target_words, \n",
    "            inputs=embed, \n",
    "            num_sampled=self.num_sampled, \n",
    "            num_classes=self.vocab_size),\n",
    "        name='loss')\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용예: word analogies ( italy - rome + france )\n",
    "\n",
    "- analogy_a = rome\n",
    "- analogy_b = italy\n",
    "- analogy_c = france\n",
    "\n",
    "```\n",
    "    analogy_a = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "    analogy_b = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "    analogy_c = tf.placeholder(dtype=tf.int32)  # [N]\n",
    "\n",
    "    # n_emb = tf.nn.l2_normalize(embed_matrix, 1)\n",
    "\n",
    "    a_emb = tf.gather(embed_matrix, analogy_a)\n",
    "    b_emb = tf.gather(embed_matrix, analogy_b)\n",
    "    c_emb = tf.gather(embed_matrix, analogy_c)\n",
    "\n",
    "    target = c_emb + (b_emb - a_emb)\n",
    "\n",
    "    dist = tf.matmul(target, embed_matrix, transpose_b=True)\n",
    "\n",
    "    # For each question (row in dist), find the top 4 words.\n",
    "    _, pred_idx = tf.nn.top_k(dist, 4)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용예: similar words\n",
    "\n",
    "- nearby_word = rome\n",
    "\n",
    "```\n",
    "    n_emb = tf.nn.l2_normalize(embed_matrix, 1)\n",
    "\n",
    "    nearby_word = tf.placeholder(dtype=tf.int32)\n",
    "    nearby_emb = tf.gather(n_emb, nearby_word)\n",
    "    nearby_dist = tf.matmul(nearby_emb, n_emb, transpose_b=True)\n",
    "    nearby_val, nearby_idx = tf.nn.top_k(\n",
    "        nearby_dist,\n",
    "        min(1000, vocab_size))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터의 준비\n",
    "\n",
    "- 예시: 나무위키 백업 파일 (json 포맷)\n",
    "\n",
    "```\n",
    "    $ du -sh namuwiki_170327.json\n",
    "    7.1G    namuwiki_170327.json\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1단계: 텍스트로 변환\n",
    "\n",
    "- json 원본\n",
    "\n",
    "<div>\n",
    "<code>\n",
    "[{\"namespace\":\"0\",\"title\":\"!\",<span style=\"color:red\">\"text\":</span>\"#redirect \\ub290\\ub08c\\ud45c\\n\",\"contributors\":[\"namubot\",\"R:hoon12560\"]},{\"namespace\":\"0\",\"title\":\"!!\\uc544\\uc557!!\",<span style=\"color:red\">\"text\"</span>:\"[[\\ud30c\\uc77c:3444050440.jpg]]\\n([[\\uc2e0 \\uc138\\uacc4\\uc218\\uc758 \\ubbf8\\uad81 2]]\\uc5d0\\uc11c \\ub72c !!\\uc544\\uc557!!)\\n{{{+1 ...\n",
    "</code>\n",
    "</div>\n",
    "\n",
    "\n",
    "- namuwiki_170327.json 파일은 항목들의 리스트로 구성되어 있는데, 각 항목은 파이썬 dictionary 같은 구조를 가짐\n",
    "  - \"namespace\"\n",
    "  - \"title\"\n",
    "  - <span style=\"color:red\">\"text\"</span> : 필요한 것은 text 필드\n",
    "  - \"contributors\"\n",
    "  - etc?\n",
    "\n",
    "- 항목 갯수는 namuwiki_170327.json 에서는 930,000 개 가량\n",
    "\n",
    "- json 파일을 파이썬에서 읽어오기 위해서는 [`json.load()`](http://devdocs.io/python~2.7/library/json#json.load) 함수 사용\n",
    "  - 하지만 파일 크기가 7.1G 넘는 크기라는 점을 감안해서 메모리 관리에 유의\n",
    "  - 읽어온 데이터는 dictionary 의 list 객체\n",
    "  - 리스트 각 항목의 \"text\" 값만 읽어서 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 단계: 메타 텍스트 처리\n",
    "\n",
    "- 나무위키의 경우는 \"위키문법\" 이라는 것에 따른 메타표기를 사용\n",
    "  - https://namu.wiki/w/%EB%82%98%EB%AC%B4%EC%9C%84%ED%82%A4:%EB%AC%B8%EB%B2%95%20%EB%8F%84%EC%9B%80%EB%A7%90\n",
    "  \n",
    "\n",
    "- 예시:\n",
    "\n",
    "```\n",
    "[include(틀:다른 뜻1, other1=말줄임표 등으로 사용하는 용어, rd1=(...))]\n",
    "\n",
    " * 상위 문서: [[개그 콘서트/종영 코너]]\n",
    "\n",
    "||<table align=right>|| http://www.mftp.info/20150901/1443910706x-576894271.jpg?width=320 ||\n",
    "|||| “……” ||\n",
    "|| 참여 프로그램 || [[개그 콘서트]] ||\n",
    "|| 시작 || [[2013년]] [[5월 26일]] ||\n",
    "|| 종료 || 2013년 [[10월 13일]] ||\n",
    "|| 출연진 || [[유민상#s-1|유민상]], [[김희원]], [[송필근]], 남궁경호 ||\n",
    "|| 유행어 || XX는 더 이상합니다[* 어색한 상황을 수습해보려는 변명이 더 안 좋은 결과를 낼 때 나오는 말.] ||\n",
    "\n",
    "--[[나는 킬러다|2년 뒤에는 사위가 장인을 죽이려한다]]--\n",
    "\n",
    "'''진짜로 코너 [[이름]]이 ([[말풍선]])“……”[* 큰 따옴표도 포함된다.](말풍선)이다'''. 언론에서는 이 코너를 언급할 때 '점점점' 또는 '점점점점점점'이라는 표현을 사용한다. 코너명의 유래는 흔히 쓰이는 [[...|말줄임표]]. 2013년 5월 26일부터 방송되었고 [[김희원]], 남궁경호, [[유민상#s-1|유민상]], [[송필근]]이 출연한다.\n",
    "\n",
    "남자친구(남궁경호)와 여자친구(김희원)의 아버지(유민상) 간의 어색함을 다룬 코미디. 코너 초창기에는 남자친구 역인 남궁경호의 비중이 높았고 송필근은 배달부나 이웃주민 등 조연 역할이었으나, 송필근이 남궁경호의 아버지 역할(즉 유민상과 사돈관계) 컨셉으로 고정되면서 남궁경호의 비중이 줄어들고[* 심지어 아예 등장하지 \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- 참고: [\"namu_wiki_db_preprocess\", 김인식](https://github.com/insikk/namu_wiki_db_preprocess)\n",
    "  - https://github.com/insikk/namu_wiki_db_preprocess\n",
    "  - 정식의 위키문법 파서(?) 를 사용하지 않고 regular expression 을 이용해서 메타문자를 제거하는 코드 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3단계: 형태소 분석을 통한 단어 추출\n",
    "\n",
    "- 형태소 분석을 통해서 불필요한 형태소를 제거\n",
    "\n",
    "\n",
    "- 파이썬용 형태소 분석기로 konlpy.tag 모듈의 분석기를 사용할 수 있음\n",
    "  - http://konlpy.org/\n",
    "\n",
    "\n",
    "- 사용 예시:\n",
    "\n",
    "```\n",
    "    >>> from konlpy.tag import Kkma\n",
    "    >>> from konlpy.utils import pprint\n",
    "    >>> kkma = Kkma()\n",
    "    >>> pprint(kkma.pos(u'오류보고는 실행환경, 에러메세지와함께 설명을 최대한상세히!^^'))\n",
    "    [(오류, NNG),\n",
    "     (보고, NNG),\n",
    "     (는, JX),\n",
    "     (실행, NNG),\n",
    "     (환경, NNG),\n",
    "     (,, SP),\n",
    "    ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- kkma.pos() 호출 결과는 (단어스트링, 품사태그) 튜플의 목록\n",
    "\n",
    "\n",
    "- 단어스트링은 어미변화등이 정규화 되어 있는 상태\n",
    "\n",
    "\n",
    "- 품사태그는 단어의 형태소 유형을 서술\n",
    "  - konlpy.tag 에서 제공하는 분석기들 마다 품사태그이름을 자체 정의해서 통일되어 있지 않다는 점에 유의\n",
    "  - N으로 시작하는 태그 이름은 명사, V로 시작하면 동사, ... 라는 식의 대략의 규칙은 일치하는 경우가 많음\n",
    "\n",
    "\n",
    "- 품사태그의 예시:\n",
    "  - http://openuiz.blogspot.kr/2016/07/mecab-ko-dic.html\n",
    "\n",
    "  > <div style=\"text-align:left\"><img  src=\"2017-10-13 11.35.49.png\" style=\"width:33.5rem\"/></div>\n",
    "\n",
    "- 형태소 분석 결과를 이용한 단어 추출\n",
    "  - 사용 목적에 따라 달라지겠지만, 단어 관계를 유추하는 목적으로 다음과 같은 규칙을 임의 설정해서 시도\n",
    "\n",
    "```\n",
    "    if  e[1].startswith(u'J') or \\\n",
    "        e[1].startswith(u'E') or \\\n",
    "        e[1].startswith(u'M') or \\\n",
    "        e[1].startswith(u'I') or \\\n",
    "        e[1].startswith(u'X') or \\\n",
    "        e[1].startswith(u'S') or \\\n",
    "        False\n",
    "        continue\n",
    "\n",
    "```\n",
    "\n",
    "- 여기서는 두 가지 결과물이 나와야 함\n",
    "  - 전체 문서에 등장하는 정규화된 단어의 빈도순 목록\n",
    "    - 일정 빈도 이하로 나타나는 단어들은 생략\n",
    "    - 단어 목록에 해당 단어가 없음을 나타내는 UNK 심볼 추가\n",
    "    - 가장 많이 등장하는 단어가 1번이 되도록 빈도수 역순으로 인덱스 부여\n",
    "  - 전체 문서 중에서 정규화된 단어들만 차례로 나열하여 다음 단계에서 사용할 입력 텍스트 파일 작성\n",
    "    - 2-pass 스캔으로 단어 인덱스 시퀀스를 만들어 두면 좋음 (.npy 포맷 등으로 저장)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트 소스\n",
    "\n",
    "- namuwiki_170327.json : 나무위키 덤프 원본\n",
    "- nb-namu-db-parse.ipynb : 나무위키 텍스트 추출 스크립트\n",
    "  - https://github.com/insikk/namu_wiki_db_preprocess 코드 기반으로 작성되었음\n",
    "- scan_count_words.py : 형태소 처리및 동사 명사 추출 스크립트\n",
    "- word2vec-demo.py : word2vec 학습/테스트 스크립트\n",
    "  - stanford tensorflow tutorial 중의 04_word2vec_visualize.py 코드 기반으로 작성되었음\n",
    "  - https://github.com/chiphuyen/stanford-tensorflow-tutorials/tree/master/examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Time for Source Code Walk-through_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제점\n",
    "\n",
    "- CS20SI 예제코드를 기반으로 해서 지나치게 단순화한 모형 사용\n",
    "- 형태소 분석을 하였지만, POS 태그 정보는 활용하지 않고 단어를 정규화 하는 용도로만 사용\n",
    "- 나무위키 문서의 특성상 일반 문서와 단어 분포가 다른게 아닌가 의심됨\n",
    "- Tensorflow 에서 Word2Vec 에 필요한 `tf.nn.embedding_lookup()` 연산에 대해서 GPU 가속 기능을 사용할 수 없음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Optimization\n",
    "\n",
    "- \"Distributed Representations of Words and Phrases and their Compositionality\", Mikolov, et. al.\n",
    "  - https://arxiv.org/pdf/1310.4546.pdf\n",
    "\n",
    "\n",
    "- 공통적으로 같이 붙여서 사용하는 단어 쌍을 찾아서 하나의 단어처럼 취급\n",
    "  > For example, “Boston Globe” is a newspaper, and so it is not a natural combination of the meanings of “Boston” and “Globe”\n",
    "  \n",
    "  \n",
    "- 빈도수 높은 단어의 subsampling\n",
    "\n",
    "\n",
    "- “Negative Sampling” : 기존의 nce_loss() 를 간략화(?) 한 cost function\n",
    "  > each training sample to update only a small percentage of the model’s weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 빈도수 높은 단어의 subsampling\n",
    "\n",
    "- 자주 등장하는 단어의 중요성을 떨어뜨려 주는 역활\n",
    "  - 영어의 \"a\", \"the\", 등\n",
    "\n",
    "- 특정한 threshold 보다 빈도수가 높은 단어를 확률 $p$ 로 랜덤하게 skip 하는 방법\n",
    "  - $P(w_i) = 1 - \\sqrt{t \\over { f(w_i) } }$\n",
    "  - $f(w_i)$ 는 ${단어수카운트} \\over {전체문서의단어수}$\n",
    "  - t 는 단어수 빈도의 threshold 값으로, $10^{-5}$ 정도의 값을 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중심단어와의 거리에 따른 weight 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제안\n",
    "\n",
    "- <span style=\"color:red\">Tensorflow 외의 방법으로 구현된 word2vec 프로그램의 사용을 추천</span>\n",
    "\n",
    "\n",
    "- Word2Vec, google\n",
    "  - https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "\n",
    "- Gensim (Python)\n",
    "  - \"한국어 Word2Vec\", 개인 블로그, theeluwin\n",
    "    - http://blog.theeluwin.kr/post/146591096133/한국어-word2vec\n",
    "\n",
    "\n",
    "- 기타 github 검색:\n",
    "  - https://github.com/search?utf8=✓&q=word2vec&type=\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
